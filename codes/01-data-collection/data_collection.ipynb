{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c23394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data gather through tweepy API\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import tweepy\n",
    "import requests\n",
    "import os\n",
    "api = pd.read_csv(r'C:\\Users\\63014\\Desktop\\twitter key.txt')\n",
    "consumer_key        = api.iloc[0,1]\n",
    "consumer_secret     = api.iloc[1,1]\n",
    "access_token        = api.iloc[3,1]\n",
    "access_token_secret = api.iloc[4,1]\n",
    "bearer_token        = api.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2995fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAISeXgEAAAAANwEgP0gtkLS%2Bjv9qCa3pIB3o1xs%3D98KYFXJI2DYHslg3StyrI2h9aB0w8PYq8iJyGS6Vdwsrhgkt7r'}\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb4c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH-0 COMPLETED;  TWEETS_COLLECTED= 100 ; TIME (s) =  0.6027121543884277\n",
      "SEARCH-10 COMPLETED;  TWEETS_COLLECTED= 1099 ; TIME (s) =  8.409476041793823\n",
      "SEARCH-20 COMPLETED;  TWEETS_COLLECTED= 866 ; TIME (s) =  16.906012535095215\n",
      "I find no information further bullyed\n",
      "SEARCH-30 COMPLETED;  TWEETS_COLLECTED= 600 ; TIME (s) =  25.869091272354126\n",
      "I find no information further bullyer\n",
      "I find no information further bulling\n"
     ]
    }
   ],
   "source": [
    "#SEARCH PARAM\n",
    "query_list = ['bullied', 'bully', 'bullyed', 'bullying', 'bullyer', 'bulling']\n",
    "# NUMBER OF TWEETS TO SEARCH \n",
    "number_of_tweets=1000 # get 1000 numbers of data\n",
    "start_time = time.time()\n",
    "max_loop_time_hrs=5\n",
    "num_tweets_collected=0\n",
    "searches=[]\n",
    "k=0\n",
    "# write a function to collect data from twitter\n",
    "for query in query_list:\n",
    "    while num_tweets_collected<number_of_tweets or (time.time()-start_time)/60./60>max_loop_time_hrs:\n",
    "        if len(searches)==0:\n",
    "            search_results = api.search_tweets(query,count=100,tweet_mode='extended') # extended to have full text\n",
    "        #ADDITIONAL SEARCHES\n",
    "        else:\n",
    "            search_results = api.search_tweets(query,count=100,max_id=max_id_next,tweet_mode='extended')\n",
    "        #UPDATE PARAMETERS\n",
    "        num_tweets_collected+=len(search_results)\n",
    "        try:\n",
    "            max_id_next=int(search_results[-1]._json[\"id_str\"])-1\n",
    "        except:\n",
    "            print('I find no information further %s'% query)\n",
    "            break\n",
    "        #SAVE SEARCH RESULTS\n",
    "        else:\n",
    "            searches.append(search_results)\n",
    "        finally:\n",
    "            if(k%10==0):\n",
    "                print(\"SEARCH-\"+str(k)+\" COMPLETED;  TWEETS_COLLECTED=\",num_tweets_collected,\"; TIME (s) = \",time.time() - start_time)\n",
    "            k+=1\n",
    "            time.sleep(0.2)\n",
    "    num_tweets_collected=0\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "386c43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(searches)):\n",
    "    for j in range(searches[i].__len__()):\n",
    "        id = searches[i][j]._json['id']\n",
    "        text = searches[i][j]._json['full_text']\n",
    "        geo = searches[i][j]._json['user']['location']\n",
    "        description = searches[i][j]._json['user']['description']\n",
    "        likes = searches[i][j]._json['favorite_count']\n",
    "        retweet_counts  = searches[i][j]._json['retweet_count']\n",
    "        lang = searches[i][j]._json['lang']\n",
    "        e = (id,text,geo,description,likes,retweet_counts,lang)\n",
    "        data.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "565a4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data,columns=['id','text','geo','description','likes','retweet_counts','lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed84370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./twitter_scrape.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
